---
title: "Tendencias IA 2026: Antes de planificar tu an팪o, lee este ana패lisis"
date: "12 de Enero de 2026"
edition: 14
source: "https://www.blog.lvrpiz.com/p/tendencias-ia-2026-analisis"
---

# Tendencias IA 2026: Antes de planificar tu an팪o, lee este ana패lisis

Gui패a completa sobre las tendencias IA 2026. Futuro de los agentes, que패 esperar de los world models y por que패 el contenido humano sera패 ma패s valioso que nunca.

Hola,

Bueno, pues ya estamos aqu칤. 칔ltimo d칤a del a침o. Llevo medio a침o con este proyecto de la newsletter y el recibimiento ha sido muy bueno. De verdad, gracias a todos los que est치is al otro lado leyendo cada semana, respondiendo y compartiendo esto.

En la edici칩n anterior os dije que si solo pod칤ais leer un post de todos los que publique este a침o, que fuera este. Y no lo dec칤a por decir. Hoy **no** quiero traeros el t칤pico resumen de *lo mejor de 2025*, porque eso ya lo hab칠is visto en mil sitios. Hoy quiero mirar hacia delante.

El tema es el futuro de la inteligencia artificial, pero poniendo el foco en **2026**. En este post os voy a soltar bastantes ideas, algunas quiz치s un poco pol칠micas, otras m치s t칠cnicas, pero todas fruto de lo que he estado observando, probando y sufriendo este 칰ltimo a침o.

Aqu칤 van mis apuestas y reflexiones para el futuro de la IA para el a침o 2026:

### **El mito de los agentes aut칩nomos (y por qu칠 2026 tampoco ser치 su a침o)**

El a침o pasado nos vendieron que 2025 ser칤a el *a침o de los agentes*. Y, ojo, a pesar de que mucha gente insista en que lo ha sido, la realidad es que ni de lejos. Una cosa es que se presente la tecnolog칤a en una demo muy bonita y otra muy distinta es que sea productiva en el mundo real.

Vimos la presentaci칩n del navegador de OpenAI, **Atlas**, que no deja de ser un *wrapper* de Google Chrome con ChatGPT siempre accesible. Nada m치s. S칤, tiene una parte ag칠ntica, pero todav칤a no es funcional del todo.

El problema de fondo es la confianza. La tecnolog칤a no est치 lo suficientemente madura como para dejarla trabajar por su cuenta sin supervisi칩n. Tanto es as칤 que las demos m치s impresionantes que nos ense침aron, esas que supuestamente tienen que dejarnos con la boca abierta, no dejaban de ser tareas simples como "hazme la compra". Y, para colmo, para agregar cuatro productos al carrito tardaba mucho m치s de lo que hubi칠ramos tardado t칰 o yo en hacerlo.

Y siempre te queda esa incertidumbre de: "Oye, 쯟o ha hecho bien? 쮿a mirado mi marca favorita? 쮿a cogido la mejor oferta?". Esa fricci칩n cognitiva mata la utilidad del agente. Mientras tengas que revisar con lupa lo que hace, no es un agente, es un becario muy r치pido pero poco fiable.

### **La trampa de los Agentes de Programaci칩n y los IDEs**

Aqu칤 hemos visto un salto que parece cualitativo, pero tiene truco. Vimos *Antigravity* de Google, hemos visto *Codex*, y pr치cticamente cada empresa de IA (m치s un mont칩n de startups que son meros *wrappers*) ha sacado su propio IDE que etiquetan como ag칠ntico.

La realidad es que de ag칠nticos tienen poco. Lo que hacen muy bien es aprovechar modelos con capacidades brutales de *Function Calling* (llamadas a herramientas), que es vital, se aprovechan de los MCPs (Model Context Protocol) y otras tecnolog칤as, pero lo que tenemos hoy son **workflows ag칠nticos**, no agentes reales.

Os cuento esto porque me pic칩 la curiosidad y hace un mes me propuse un reto: crear desde cero una aplicaci칩n sencilla sin escribir yo ni una sola l칤nea de c칩digo, usando solo *Antigravity* y los mejores modelos del momento. He aprendido una barbaridad, pero **mi conclusi칩n es que estos modelos siguen siendo orquestadores de funciones**.

Es como un modo edici칩n supervitaminado donde t칰 pides algo y el modelo es capaz de listar ficheros, eliminar cosas o editar directorios porque le hemos dado permisos, pero no tiene la autonom칤a real para tomar decisiones complejas de arquitectura o corregir el rumbo si se equivoca profundamente. **No tienen todav칤a capacidad de ver un proyecto como un todo** por mucho que haya gente que te diga que con varios ficheros de instrucciones y contexto lo tienes solucionado. Solucionan errores provocando otros en otras partes del c칩digo a pesar de usar ficheros de contexto especiales y buen prompting.

2026 mejorar치 la llamada a funciones y los modelos podr치n razonar durante m치s tiempo antes de actuar, pero seguir치n fallando estrepitosamente en tareas de largo recorrido sin intervenci칩n humana.

### **La gran deuda t칠cnica: Memoria y Contexto**

Si hay algo que tiene que mejorar s칤 o s칤, es la memoria. Actualmente, es uno de los puntos m치s d칠biles. Las memorias que usamos ahora mismo son, b치sicamente, parches que ocupan contexto del modelo.

Cuando empiezas un chat nuevo, tienes el *System Prompt*, y a eso se le a침aden ciertas memorias que el modelo ha guardado de conversaciones anteriores. El problema es que **esas memorias consumen tokens de entrada**. Cuantas m치s cosas recuerde de ti, menos espacio le queda para procesar la tarea actual, y a veces eso hace que las respuestas sean m치s personalizadas pero menos precisas t칠cnicamente.

Lo ideal, y lo que creo que veremos madurar en 2026, es un sistema donde el modelo sepa inyectar en tiempo real esas memorias sin necesidad de ocupar todo el contexto de golpe. Ya hay bastantes *papers* acad칠micos de finales de este a침o apuntando a arquitecturas de memoria din치mica. **2026 ser치 el a침o donde la memoria mejore cuantitativamente**, permitiendo que la IA mantenga el hilo de proyectos largos sin alucinar o olvidar lo que le dijiste hace tres d칤as. Aqu칤 Antrhopic lleva ventaja y tiene un modelo de memoria superior a la de otras empresas, os dejo este [post](https://simonwillison.net/2025/Sep/12/claude-memory/) de Simon Willison donde lo explica.

### **V칤deo, Imagen y el rechazo social**

En imagen y v칤deo, 2025 ha sido una locura. No es el campo que sigo con m치s especialidad, pero hay que reconocer que modelos como *Flux* o lo que est치 haciendo *Sora* o *Veo 3* son impresionantes. Para 2026, veremos modelos de v칤deo que entiendan mucho mejor las f칤sicas y que obedezcan mejor nuestras instrucciones

Y aqu칤 viene mi *hot take*: creo que **en 2026 veremos la primera serie o pel칤cula un poco** ***mainstream*** **hecha 칤ntegramente con IA**, seguramente en alguna plataforma de streaming. Pero, y aqu칤 est치 la clave, creo que tendr치 un rechazo enorme.

Ya est치 pasando. Fijaos en los anuncios de esta [Navidad de Coca-Cola](https://www.youtube.com/watch?v=Yy6fByUmPuE) o [McDonald's](https://www.youtube.com/watch?v=E-YwjXEVGo8) hechos con IA. La gente los ha puesto a parir. Y es normal. Todav칤a somos capaces de identificar esos fallos, esa falta de alma, y nos produce rechazo. Pero estamos llegando a ese punto peligroso donde dejar치 de ser distinguible. El problema no ser치 t칠cnico, ser치 social: **nos vamos a cansar de la perfecci칩n sint칠tica**. 2026 ser치 definitivamente el a침o del v칤deo t칠cnico, pero tambi칠n el a침o de la fatiga visual por IA.

### **World Models: El texto NO es suficiente**

Hay una barrera que los LLMs no van a poder cruzar con facilidad y es poder interactuar con el mundo f칤sico. Las m치quinas, a corto y medio plazo, no van a ser tan precisas movi칠ndose por el mundo real como nosotros. Un LLM sabe predecir la siguiente palabra, **pero no entiende la gravedad o que si suelta un vaso, se rompe, a menos que lo haya le칤do**.

Por eso, a partir de 2026, gran parte de los esfuerzos de investigaci칩n (y de la pasta) se ir치n a los [**World Models**](https://www.nvidia.com/en-us/glossary/world-models/). De hecho, varios investigadores top ya est치n dejando de lado los LLMs puros, diciendo que el trabajo ah칤 ya es residual o de optimizaci칩n, y se est치n [pasando a modelos que simulan entornos f칤sicos](https://techcrunch.com/2025/12/19/yann-lecun-confirms-his-new-world-model-startup-reportedly-seeks-5b-valuation/). Estos modelos ayudar치n a los robots a entender el entorno.

Pero hasta que eso llegue, va a pasar un tiempo largo. Y entre medias, va a ocurrir algo curioso que ya estamos viendo: estamos delegando la carga cognitiva a las m치quinas, y las m치quinas nos est치n ordenando el trabajo manual a nosotros. **Somos la interfaz de los LLMs en el mundo f칤sico**.

Pensadlo con *Glovo* o *Uber*. Hay una IA (no un LLM, pero un algoritmo complejo) que decide qui칠n recoge qu칠, d칩nde y cu치ndo. Optimiza la ruta. Y los humanos somos los que ejecutamos la tarea f칤sica de mover el paquete. **Somos la interfaz de la m치quina con el mundo real**. Seguiremos siendo el middleware f칤sico hasta que la rob칩tica, impulsada por estos World Models, est칠 lista. Y para eso queda m치s que para 2026.

### **La teor칤a del Internet Muerto y el valor de lo humano**

Este a침o, en el evento del *VallTechSummit*, coment칠 una noticia que me dio que pensar: la [teor칤a del Internet Muerto](https://www.galaxy.com/insights/perspectives/dead-internet-theory-collapse-online-truth) cada vez tiene m치s sentido. Esta teor칤a dice que, en el futuro, la mayor칤a del tr치fico y contenido en internet ser치 generado por bots interactuando con otros bots, y los humanos seremos una minor칤a.

En 2025 ya cruzamos una frontera hist칩rica: [la cantidad de contenido generado por IA super칩 al contenido generado por humanos](https://graphite.io/five-percent/more-articles-are-now-created-by-ai-than-humans). Esto nos lleva a dos vertientes para 2026.

La primera es la identificaci칩n. **Necesitamos saber qu칠 es humano y qu칠 no**. En otro post os habl칠 de [*SynthID*](https://www.blog.lvrpiz.com/p/guia-synthid-watermarking-texto-imagen-video) [de Google](https://www.blog.lvrpiz.com/p/guia-synthid-watermarking-texto-imagen-video), una soluci칩n t칠cnica muy buena para marcar (watermarking) texto, imagen y v칤deo. A las propias empresas de IA les interesa esto porque si entrenan a sus futuros modelos con datos generados por sus modelos anteriores, se produce un colapso de la informaci칩n, se vuelven repetitivos. Necesitan filtrar para entrenar solo con datos humanos frescos. Veremos marcas de *Generado por IA* en Instagram, Facebook, WhatsApp y LinkedIn (donde ya est치). Twitter... bueno, Twitter es otro mundo, aunque **la UE seguramente meta mano ah칤 con regulaciones** a no tardar.

En cuanto al contenido, circula una teor칤a sobre el [protocolo x402](https://www.x402.org) que vendr칤a a proteger el contenido humano en internet haciendo pagar una tasa a los bots que leen webs para entrenar modelos. Este protocolo exigir칤a a los agentes de IA pagar por el acceso a la API en tiempo real. No creo que esto se convierta en un est치ndar ni de lejos pero es curioso como hay esfuerzos en seguir protegiendo el contenido humano.

La segunda vertiente es justo esto mismo. **El contenido humano tendr치 cada vez m치s valor (o precio)**. Las experiencias presenciales, los encuentros f칤sicos, la opini칩n real de una persona con cara y ojos... eso va a cotizar al alza. La gente se va a cansar del contenido infinito y perfecto pero vac칤o.

### **Apple Intelligence y la falsa necesidad de un Chatbot**

Aqu칤 quiero hacer un inciso importante con Apple. Se les ha criticado much칤simo este a침o por no tener su propio ChatGPT, como si eso fuera el 칰nico est치ndar de IA v치lido. Pero ojo, que la gente se olvida de que Apple lleva a침os integrando *machine learning* en todo su sistema operativo y, de hecho, sus chips son los que mejor preparados est치n ahora mismo, con diferencia, para correr cargas de IA en local y creo que aunque lleguen tarde la idea que proponen es realmente buena.

Y aqu칤 lanzo la pregunta: Realmente necesitan su propio ChatGPT? Mi opini칩n es que no. Su *approach* es completamente diferente y mucho m치s ambicioso. Piensa que a ChatGPT le tienes que dar t칰 el contexto, tienes que explicarle las cosas, Apple no quiere ir por esa v칤a. Apple quiere (y puede) usar modelos fundacionales dise침ados para entender tu contexto personal al m치ximo. **Ellos ya tienen la pieza que les falta a los dem치s: saben tus citas, tus eventos, tus correos y tus h치bitos.**

No creo que Apple vaya a sacar una *app* para competir con ChatGPT ni a corto ni a medio plazo: para las preguntas gen칠ricas ya tienen el acuerdo con OpenAI y uno futuro con Google. Su apuesta real es crear un asistente proactivo que funcione solo, que se anticipe y que entienda al usuario profundamente sin que le tengas que escribir un *prompt*. Esa es la verdadera IA personal.

### **Hardware y Wearables: Realmente necesitamos llevar un collar inteligente?**

Hay rumores fuertes de que OpenAI est치 trabajando con Jony Ive en un [dispositivo](https://www.bloodinthemachine.com/p/an-always-on-openai-device-is-a-massive) [*hardware*](https://www.bloodinthemachine.com/p/an-always-on-openai-device-is-a-massive). Se habla de algo tipo collar, con micr칩fono y altavoz, para ir hablando con la IA todo el d칤a.

Concepto del posible dispositivo de OpenAI

Mira, yo aqu칤 soy muy esc칠ptico. El **tel칠fono m칩vil es una tecnolog칤a demasiado madura y perfecta como para que un** ***wearable*** **sin pantalla lo reemplace**. Adem치s, hay un factor social que se nos olvida: hablar con una m치quina en p칰blico es raro (por ahora).

La mayor칤a de nosotros pasamos el d칤a en oficinas, transporte p칰blico o espacios compartidos. No vas a ponerte a dictarle a tu collar un correo o a preguntarle dudas existenciales delante de tu compa침ero de mesa. La voz como interfaz principal est치 destinada al fracaso en entornos sociales, al menos a corto plazo.

Para m칤, **todo esto deber칤a ser una** ***app*****, no un dispositivo**. Si Apple o Google integran bien sus asistentes en el m칩vil, el dispositivo dedicado pierde todo el sentido.

### **El hardware como foso defensivo y software como commodity**

Pensadlo un momento: en un mundo donde cualquiera puede generar c칩digo y crear "software" casi al instante, **tener una app deja de ser una ventaja competitiva. El software se est치 convirtiendo en una commodity**.

Entonces, 쯗칩nde est치 el valor real? **En atar ese software a algo f칤sico**. El hardware se convierte en el nuevo foso defensivo para las empresas. Las compa침칤as que realmente van a destacar en 2026 no son las que te den otro chatbot, sino las que integren verticalmente de forma que sea imposible de copiarlas.

Mirad el 칠xito de [Oura](https://ouraring.com/es?srsltid=AfmBOooYheRn4QTBXMCKSuckgU9AULFsMXwvhVQP3EThVkBE7AvNvuX-) o [Whoop](https://www.whoop.com/es/es/?srsltid=AfmBOoqyFvSeH27rCD5hgIRnErHboOAjiYJ8_v3UBlmE9Gl5-z9f3MhX). La IA ah칤 es brutal, s칤, pero est치 casada a fuego con el anillo o la pulsera. O casos m치s de nicho como [Board](https://board.fun) para juegos f칤sicos conectados o [Meter](https://www.meter.com) en redes empresariales. No te venden solo el panel de control con IA, te venden el equipo industrial necesario para que eso funcione. Esa combinaci칩n es mucho m치s dif칤cil de replicar para un competidor que simplemente clonar un repo de GitHub.

Adem치s aqu칤 hay un cambio importante, cada vez es m치s f치cil fabricar. Est치n surgiendo un mont칩n de empresas que simplifican el dise침o de chips, los prototipos y la gesti칩n de la cadena de suministro. Montar una startup de hardware sol칤a ser una locura financiera, ahora empieza a ser viable. As칤 que preparaos **para ver mucha m치s variedad de productos f칤sicos espec칤ficos**, y menos SaaS gen칠ricos que no aportan nada nuevo.

### **La burbuja econ칩mica: 쮻칩nde est치 el dinero real?**

Para cerrar, una reflexi칩n econ칩mica. 쮼stamos en una burbuja de IA? Depende de d칩nde mires. En el *hardware* no hay burbuja: Nvidia y los fabricantes de chips est치n haciendo caja real. Es dinero f칤sico circulando ah칤 hay negocio tangible.

Donde s칤 veo una **burbuja es en la capa de aplicaci칩n**. Hay miles de empresas que por el simple hecho de ponerse un *AI* en el nombre y ser un *wrapper* b치sico de ChatGPT se han valorado por m칰ltiplos absurdos. Esas empresas, que no tienen barrera de entrada tecnol칩gica, van a desaparecer. En Estados Unidos est치n saliendo como setas y caer치n igual de r치pido. Solo sobrevivir치n las que aporten valor real sobre el modelo, no las que solo lo revenden.

Dicho esto, el a침o que viene por estas fechas nos pasamos de nuevo por aqu칤 y repasamos juntos si me he equivocado mucho, que seguro que alguna patinada hay.

Gracias por estar ah칤 este 2025. Buena entrada de a침o 2026 游볙

Abrazo, 츼lvaro